This project fine-tunes a RoBERTa-base model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA to classify text into four AG News categories: World, Sports, Business, and Sci/Tech. The training is carefully designed to keep under 1 million trainable parameters, use cosine learning rate decay with warmup, and achieve strong validation scores. We freeze the base model, apply LoRA on the attention modules (query, value), and monitor metrics like Accuracy, Precision and Recall during training and evaluation. 
